{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Custom models\n",
    "from prepro import readfile, get_sentence, is_number, extract_words, partial_tags\n",
    "\n",
    "\n",
    "#most common entity per word\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from my github repo\n",
    "train = readfile(\"train.txt\")\n",
    "corpus = train.copy()\n",
    "test = readfile(\"test.txt\")\n",
    "\n",
    "#create corpus\n",
    "corpus.extend(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "tags = []\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        words.append(word[0])\n",
    "        tags.append(word[1])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = partial_tags(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dataset:  250056\n",
      "Number of tags in the dataset:  5\n"
     ]
    }
   ],
   "source": [
    "words=list(words)\n",
    "n_words = len(words)\n",
    "n_tags = len(set(tags))\n",
    "\n",
    "labels=list(set(tags))\n",
    "labels.remove('O')\n",
    "\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "print(\"Number of tags in the dataset: \", n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'rejects' is identified by the index: 218128\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "print(\"The word 'rejects' is identified by the index: {}\".format(word2idx[\"rejects\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels LOC (location) is identified by the index: 250026\n"
     ]
    }
   ],
   "source": [
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "print(\"The labels LOC (location) is identified by the index: {}\".format(tag2idx[\"LOC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word_list):\n",
    "    new_list= []\n",
    "    for word in word_list:\n",
    "        if is_number(word2idx[word]):\n",
    "            new_list.append(word2idx[word])\n",
    "    else:\n",
    "        None\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set\n",
    "train_words, train_tags = extract_words(train)\n",
    "train_tags=partial_tags(train_tags)\n",
    "\n",
    "#tokenize words into tokens\n",
    "tr_words = tokenize(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTagger(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Expects a list of words as X and a list of tags as y.\n",
    "        '''\n",
    "        voc = {}\n",
    "        self.tags = []\n",
    "        for x, t in zip(X, y):\n",
    "            if t not in self.tags:\n",
    "                self.tags.append(t)\n",
    "            if x in voc:\n",
    "                if t in voc[x]:\n",
    "                    voc[x][t] += 1\n",
    "                else:\n",
    "                    voc[x][t] = 1\n",
    "            else:\n",
    "                voc[x] = {t: 1}\n",
    "        self.memory = {}\n",
    "        for k, d in voc.items():\n",
    "            self.memory[k] = max(d, key=d.get)\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        '''\n",
    "        Predict the the tag from memory. If word is unknown, predict 'O'.\n",
    "        '''\n",
    "        return [self.memory.get(x, 'O') for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to NER training set \n",
    "tagger = MemoryTagger()\n",
    "tagger.fit(tr_words, train_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "#predict the first sentence using the base model\n",
    "print(get_sentence(train,1))\n",
    "print(tagger.predict(tokenize(get_sentence(train,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model to get train prediction\n",
    "pred = list(cross_val_predict(estimator=MemoryTagger(), X=tr_words, y=train_tags, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.87      0.72      0.78      4593\n",
      "         LOC       0.85      0.77      0.81      8297\n",
      "         ORG       0.82      0.55      0.66     10025\n",
      "         PER       0.95      0.63      0.76     11128\n",
      "\n",
      "   micro avg       0.87      0.65      0.75     34043\n",
      "   macro avg       0.87      0.67      0.75     34043\n",
      "weighted avg       0.88      0.65      0.74     34043\n",
      "\n",
      "F1 Score: 0.7445045871496001\n"
     ]
    }
   ],
   "source": [
    "#generate report on entire model\n",
    "report = classification_report(y_pred=pred, y_true=train_tags,labels=labels)\n",
    "f1= f1_score(y_pred=pred, y_true=train_tags,average='weighted',labels=labels)\n",
    "print(report)\n",
    "print(\"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the test data set\n",
    "test_words, test_tags = extract_words(test)\n",
    "test_tags=partial_tags(test_tags)\n",
    "\n",
    "#tokenize words into tokens\n",
    "te_words = tokenize(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model to get train prediction\n",
    "pred = cross_val_predict(estimator=MemoryTagger(), X=te_words, y=test_tags, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.89      0.54      0.67       918\n",
      "         LOC       0.84      0.64      0.72      1925\n",
      "         ORG       0.77      0.38      0.51      2496\n",
      "         PER       0.94      0.30      0.46      2773\n",
      "\n",
      "   micro avg       0.84      0.43      0.57      8112\n",
      "   macro avg       0.86      0.47      0.59      8112\n",
      "weighted avg       0.86      0.43      0.56      8112\n",
      "\n",
      "0.5725688222837596\n"
     ]
    }
   ],
   "source": [
    "#generate report on entire model\n",
    "report = classification_report(y_pred=pred, y_true=test_tags,labels=labels)\n",
    "f1= f1_score(y_pred=pred, y_true=test_tags,average='micro',labels=labels)\n",
    "print(report)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
